{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Translation and Deployment: Serve an ONNX Model via GraphPipe\n",
    "\n",
    "Also see the [official TensorFlow Serving overview](https://www.tensorflow.org/serving/overview)\n",
    "\n",
    "In this section, I will show how to serve our trained and exported TensorFlow model through TensorFlow Serving that is an integral part of TensorFlow. We will run TF Serving within a Docker container to separate environments and to facilitate easy horizontal scaling via Kubernetes afterwards.\n",
    "\n",
    "Make sure to have the following technologies available on your machine by following [these](https://www.tensorflow.org/serving/setup) instructions:\n",
    "* [Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-docker-ce)\n",
    "* [Using TensorFlow Serving via docker](https://www.tensorflow.org/serving/docker)\n",
    "* Pull the TensorFlow docker image: ```docker pull tensorflow/serving```\n",
    "\n",
    "![TensorFlow Serving Architecture](img/tf_serving_architecture.svg)\n",
    "\n",
    "First, let's have a look at the different **components of TF Serving**:\n",
    "1. **Servables**\n",
    "    * underlying objects that may encompass part of a model, a model or many models, a servable has a version\n",
    "    * clients can either request the latest or some specific version of a servable\n",
    "    * they allow for more than one version to be loaded concurrently\n",
    "    * enables gradual rollout of models \n",
    "    * models are represented as one or more servables\n",
    "    \n",
    "2. **Loaders**\n",
    "    * manage a servable's *lifecycle*\n",
    "        * load a servable\n",
    "        * unload a servable\n",
    "        \n",
    "3. **Sources**\n",
    "    * plugin modules\n",
    "        * find and provide servables\n",
    "    * *aspired version*: servable version that should be loaded and ready\n",
    "    \n",
    "4. **Manager**\n",
    "    * handles the full lifecycle of servables (loading, serving, unloading)\n",
    "    * listens to sources and tracks versions\n",
    "    * on client request returns a handle to servable\n",
    "    \n",
    "5. **Core**\n",
    "    * manages lifecycle and metrics of servables\n",
    "    * treats servables as opaque objects\n",
    "    \n",
    "    \n",
    "*How does the model serving process work?*\n",
    "\n",
    "Running a model server, we tell the manager to what source to listen, e.g. a file system. Saving a trained TensorFlow model to that file system triggers the manager to recognize it as a new servable. The servable becomes an aspired version. A loader instance is made ready to load the model and tells the manager the amount of resources required for loading. The manager handles resources and decides when to let the loader load the model. Depending on the configuration it may unload an older version of that servable or keep it. After successful loading the manager can starts serving client requests returning handles to the very new servable or other, if explicitly requested.\n",
    "\n",
    "**Enough Theory, let's turn that knowledge into practice!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.saved_model.utils import build_tensor_info\n",
    "from tensorflow.python.saved_model import signature_def_utils\n",
    "from tensorflow.python.saved_model import signature_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emnist_dl2prod.utils import eval_serving_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = '../models/dnn_model_pt.onnx'\n",
    "dnn_model_onnx = onnx.load(onnx_model_path)\n",
    "dnn_model_tf = prepare(dnn_model_onnx, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = '../models/tf_emnist/'\n",
    "if os.path.exists(export_path):\n",
    "    shutil.rmtree(export_path)\n",
    "\n",
    "model_version = 1\n",
    "model_name = 'tf_emnist'\n",
    "model_path = os.path.join('..', 'models', model_name, str(model_version))\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does our ONNX-TF model comprise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External Input: ['flattened_rescaled_img_28x28']\n",
      "External Output: ['softmax_probabilities']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'weight_1': <tf.Tensor 'Const:0' shape=(512, 784) dtype=float32>,\n",
       " 'bias_1': <tf.Tensor 'Const_1:0' shape=(512,) dtype=float32>,\n",
       " 'weight_2': <tf.Tensor 'Const_2:0' shape=(256, 512) dtype=float32>,\n",
       " 'bias_2': <tf.Tensor 'Const_3:0' shape=(256,) dtype=float32>,\n",
       " 'weight_3': <tf.Tensor 'Const_4:0' shape=(62, 256) dtype=float32>,\n",
       " 'bias_3': <tf.Tensor 'Const_5:0' shape=(62,) dtype=float32>,\n",
       " 'flattened_rescaled_img_28x28': <tf.Tensor 'flattened_rescaled_img_28x28:0' shape=(1, 784) dtype=float32>,\n",
       " '7': <tf.Tensor 'add:0' shape=(1, 512) dtype=float32>,\n",
       " '8': <tf.Tensor 'add_1:0' shape=(1, 512) dtype=float32>,\n",
       " '9': <tf.Tensor 'add_2:0' shape=(1, 256) dtype=float32>,\n",
       " '10': <tf.Tensor 'add_3:0' shape=(1, 256) dtype=float32>,\n",
       " '11': <tf.Tensor 'add_4:0' shape=(1, 62) dtype=float32>,\n",
       " 'softmax_probabilities': <tf.Tensor 'Softmax:0' shape=(1, 62) dtype=float32>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"External Input: {}\".format(dnn_model_tf.predict_net.external_input))\n",
    "print(\"External Output: {}\".format(dnn_model_tf.predict_net.external_output))\n",
    "dnn_model_tf.predict_net.tensor_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signature Building and Adding them to the Model\n",
    "(this was already done by GraphPipe before)\n",
    "\n",
    "also see [SignatureDefs in SavedModel for TensorFlow Serving](https://www.tensorflow.org/serving/signature_defs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Obtain TensorInfo objects for Model Input and Output Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = dnn_model_tf.predict_net.graph.get_tensor_by_name(\n",
    "                            'flattened_rescaled_img_28x28:0')\n",
    "output_tensor = dnn_model_tf.predict_net.graph.get_tensor_by_name('Softmax:0')\n",
    "\n",
    "input_tensor_info = build_tensor_info(input_tensor)\n",
    "output_tensor_info = build_tensor_info(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"flattened_rescaled_img_28x28:0\"\n",
       "dtype: DT_FLOAT\n",
       "tensor_shape {\n",
       "  dim {\n",
       "    size: 1\n",
       "  }\n",
       "  dim {\n",
       "    size: 784\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"Softmax:0\"\n",
       "dtype: DT_FLOAT\n",
       "tensor_shape {\n",
       "  dim {\n",
       "    size: 1\n",
       "  }\n",
       "  dim {\n",
       "    size: 62\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build classification signature\n",
    "As this did not work with just a sole classification signature, we try to combine it with an additional prediction signatures - as described in the [MNIST example]:(https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/mnist_saved_model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_signature = (\n",
    "    signature_def_utils.build_signature_def(\n",
    "        inputs={\n",
    "            signature_constants.CLASSIFY_INPUTS:\n",
    "                input_tensor_info\n",
    "        },\n",
    "        outputs={\n",
    "            signature_constants.CLASSIFY_OUTPUT_SCORES:\n",
    "                output_tensor_info\n",
    "        },\n",
    "        method_name=signature_constants.CLASSIFY_METHOD_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs {\n",
       "  key: \"inputs\"\n",
       "  value {\n",
       "    name: \"flattened_rescaled_img_28x28:0\"\n",
       "    dtype: DT_FLOAT\n",
       "    tensor_shape {\n",
       "      dim {\n",
       "        size: 1\n",
       "      }\n",
       "      dim {\n",
       "        size: 784\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "outputs {\n",
       "  key: \"scores\"\n",
       "  value {\n",
       "    name: \"Softmax:0\"\n",
       "    dtype: DT_FLOAT\n",
       "    tensor_shape {\n",
       "      dim {\n",
       "        size: 1\n",
       "      }\n",
       "      dim {\n",
       "        size: 62\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "method_name: \"tensorflow/serving/classify\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build prediction signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_signature = (\n",
    "    signature_def_utils.build_signature_def(\n",
    "        inputs={'images': input_tensor_info},\n",
    "        outputs={'scores': output_tensor_info},\n",
    "        method_name=signature_constants.PREDICT_METHOD_NAME\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs {\n",
       "  key: \"images\"\n",
       "  value {\n",
       "    name: \"flattened_rescaled_img_28x28:0\"\n",
       "    dtype: DT_FLOAT\n",
       "    tensor_shape {\n",
       "      dim {\n",
       "        size: 1\n",
       "      }\n",
       "      dim {\n",
       "        size: 784\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "outputs {\n",
       "  key: \"scores\"\n",
       "  value {\n",
       "    name: \"Softmax:0\"\n",
       "    dtype: DT_FLOAT\n",
       "    tensor_shape {\n",
       "      dim {\n",
       "        size: 1\n",
       "      }\n",
       "      dim {\n",
       "        size: 62\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "method_name: \"tensorflow/serving/predict\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add Signatures to Model and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "[2018-10-18 11:15:22] INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "[2018-10-18 11:15:22] INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ../models/tf_emnist/1/saved_model.pb\n",
      "[2018-10-18 11:15:22] INFO:tensorflow:SavedModel written to: ../models/tf_emnist/1/saved_model.pb\n",
      "Done exporting!\n"
     ]
    }
   ],
   "source": [
    "with dnn_model_tf.predict_net.graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        builder.add_meta_graph_and_variables(\n",
    "          sess, [tf.saved_model.tag_constants.SERVING],\n",
    "          signature_def_map={\n",
    "              'predict_images':\n",
    "                  prediction_signature,\n",
    "              signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n",
    "                  classification_signature,\n",
    "          },\n",
    "          main_op=tf.tables_initializer(),\n",
    "          strip_default_attrs=True)\n",
    "        builder.save()\n",
    "        print(\"Done exporting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the model up running with Docker and use the REST API (instead of gRPC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* follow [this](https://www.tensorflow.org/serving/docker) explanation on using TensorFlow Serving via Docker which basically contains two steps:\n",
    "\n",
    "\n",
    "1. Install docker\n",
    "2. Pull the TensorFlow Serving image: `docker pull tensorflow/serving`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* stop and remove eventually existing docker container with name `emnist_model`\n",
    "\n",
    "```docker stop emnist_model```\n",
    "\n",
    "```docker rm emnist_model```\n",
    "\n",
    "* start docker container for port 8501 and our model:\n",
    "```\n",
    "docker run -p 8501:8501 \\\n",
    "--name emnist_model --mount type=bind,source=$(pwd)/../models/tf_emnist,target=/models/tf_emnist \\\n",
    "-e MODEL_NAME=tf_emnist -t tensorflow/serving:1.10.1 &\n",
    "```\n",
    "\n",
    "which executed the following within the container image:\n",
    "```\n",
    "tensorflow_model_server --port=8500 --rest_api_port=8501 \\\n",
    "  --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME}\n",
    "```\n",
    "\n",
    "default values:\n",
    "\n",
    "* MODEL_BASE_PATH: models\n",
    "* MODEL_NAME: model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This leaves us with the followig code that looks promising:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018-10-12 13:38:15.518130: I tensorflow_serving/model_servers/server.cc:82] Building single TensorFlow model file config:  model_name: tf_emnist model_base_path: /models/tf_emnist\n",
    "2018-10-12 13:38:15.518416: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.\n",
    "2018-10-12 13:38:15.518455: I tensorflow_serving/model_servers/server_core.cc:517]  (Re-)adding model: tf_emnist\n",
    "2018-10-12 13:38:15.638251: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: tf_emnist version: 1}\n",
    "2018-10-12 13:38:15.638370: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: tf_emnist version: 1}\n",
    "2018-10-12 13:38:15.638411: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: tf_emnist version: 1}\n",
    "2018-10-12 13:38:15.639975: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:360] Attempting to load native SavedModelBundle in bundle-shim from: /models/tf_emnist/1\n",
    "2018-10-12 13:38:15.641451: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /models/tf_emnist/1\n",
    "2018-10-12 13:38:15.659090: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n",
    "2018-10-12 13:38:15.660035: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
    "2018-10-12 13:38:15.672728: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:162] Restoring SavedModel bundle.\n",
    "2018-10-12 13:38:15.673671: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:172] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: /models/tf_emnist/1/variables/variables.index\n",
    "2018-10-12 13:38:15.673710: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:138] Running MainOp with key saved_model_main_op on SavedModel bundle.\n",
    "2018-10-12 13:38:15.677101: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:259] SavedModel load for tags { serve }; Status: success. Took 35653 microseconds.\n",
    "2018-10-12 13:38:15.678135: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:83] No warmup data file found at /models/tf_emnist/1/assets.extra/tf_serving_warmup_requests\n",
    "2018-10-12 13:38:15.684767: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: tf_emnist version: 1}\n",
    "2018-10-12 13:38:15.686409: I tensorflow_serving/model_servers/server.cc:285] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
    "[warn] getaddrinfo: address family for nodename not supported\n",
    "2018-10-12 13:38:15.686843: I tensorflow_serving/model_servers/server.cc:301] Exporting HTTP/REST API at:localhost:8501 ...\n",
    "[evhttp_server.cc : 235] RAW: Entering the event loop ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a model status request\n",
    "```GET http://host:port/v1/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"http://localhost:8501/v1/models/tf_emnist\")\n",
    "print(\"HTTP Response Status Code: {}\".format(r.status_code))\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't seem to work yet ```¯\\_(ツ)_/¯````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a test query with random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_img = np.random.uniform(size=(1,784))\n",
    "data_payload = {\"instances\": random_img.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(\"http://localhost:8501/v1/models/tf_emnist:predict\",\n",
    "                  data=json.dumps(data_payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HTTP Response Status Code: {}\".format(r.status_code))\n",
    "print(\"HTTP Response content as json:\")\n",
    "print(r.json())\n",
    "print(\"Class Index Prediction: {}\".format(np.argmax(r.json()['predictions'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Prediction Accuracy using our test set samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "request_url = \"http://localhost:8501/v1/models/tf_emnist:predict\"\n",
    "_ = eval_serving_performance(n_examples=1000, n_print_examples=10,\n",
    "                             request_url=request_url, dataset='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
